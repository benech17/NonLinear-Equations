\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{amsfonts}
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=blue,  %choose some color if you want links to stand out
}
\newcommand{\R}{\mathbb{R}}

\author{Benamara Abdelkader \\ Benichou Yaniv }
\date{08 Juin 2020}

\begin{document}
\title{Projet maths-info \\ 
Résolution d'équations non-linéaires}
\maketitle

\newpage
\tableofcontents
\newpage

\section{Introduction}
    Le but de ce projet est de décrire des algorithmes les plus fréquemment utilisés pour résoudre des équations non linéaires du type : \\ f(x) = 0.
    Ainsi, l'objet essentiel de ce chapitre est l’approximation des racines d’une fonction réelle d’une variable réelle, c’est-à-dire la résolution approchée du problème suivant :
    étant donné  $f : I =]a, b[ \subset R \rightarrow R$ , trouver $\alpha \in \pmb{\mathbb{R}}$ \quad tel que  $f(\alpha) = 0$. \\
 Les méthodes que nous verrons ici sont  itératives, c'est à dire qu'elles consistent à construire une suite $x_k$ telle que $\lim\limits_{k \rightarrow +\infty} x^k = \alpha$ . 
 Ainsi, au dela d'approximer une solution, nous nous interresseront également à la vitesse de convergence,s'il y a convergence ou non,d'une méthode pour une fonction donnée. 
 Pour cela, nous définissons la convergence des itérations par la notion suivante :
\quaddOn dit qu’une suite $x^k$ construite par une méthode numérique converge vers $\alpha$ avec un ordre p \geq 1 si \quad $\exists C>0 : \frac{| x^{k+1} - \alpha |}{|x^k - \alpha |^p } \leq C ,\quad  \forall k \geq k_0 .$

où $k_0 \geq 0$ est un entier. Dans ce cas, on dit que la méthode est d’ordre p. Remarquer que si p est égal à 1, il est nécessaire que C soit inférieur à  1 .
pour que $x_k$ converge vers $\alpha$. On appelle alors la constante C facteur de convergence de la méthode.
\newpage


\section{Méthode De Dichotomie}
\subsection{Théoreme des valeurs intermédiaires :}
Pour toute application continue $ f : [a, b] \to \R $ et tout reel u compris entre f(a) et f(b), il existe au moins un reel c compris entre a et b tel que f(c) = u. \\
En particulier on a la version de Bolzano qui nous interesse précisement.

\subsection{Théoreme de Bolzano}
Soit une fonction continue $ f : [a, b] \to \R $ \\
si $ f(a)f(b) < 0 $  alors $ \exists \alpha \in  ]a, b[ $ tel que f($\alpha$)=0. \\ 


On considère deux nombres réels a et b et une fonction réelle f continue sur l'intervalle [a, b] telle que f(a) et f(b) soient de signes opposés. Supposons que nous voulions résoudre l'équation f(x) = 0. D'après le théorème des valeurs intermédiaires, f a au moins un zéro dans l’intervalle [a, b]. La méthode de dichotomie consiste à diviser l’intervalle en deux en calculant m = (a+b) / 2. \\
Il y a maintenant deux possibilités : ou f(a) et f(m) sont de signes contraires, ou f(m) et f(b) sont de signes contraires.

L’algorithme de dichotomie est alors appliqué au sous-intervalle dans lequel le changement de signe se produit, ce qui signifie que l’algorithme de dichotomie est récursif.
\subsection{L'algorithme de dichotomie}
\begin{algorithm}[H]
\SetAlgoLined
\KwResult{millieu $\:$ tq $\:$ f(millieu)=0}
 initialization\;
 \While{fin-debut > err}{
  millieu = (debut + fin ) / 2\;
  \eIf{f(millieu) >  0}{
   fin = millieu\;
   }{
   debut=millieu\;
  }
 }
 \caption{Méthode de dichotomie}
\end{algorithm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\newpage
\begin{lstlisting}[language=Python, caption=Méthode de dichotomie en Python]
class Dichotomie(Equa_Solver):

    def solve(self):
        # Donnees en parametres
        a , b = self.a , self.b
        err=self.err
        try:
            f = lambda x: eval(str(self.f))
        except (TypeError,SyntaxError):
            return
        x_list=[]

        print(f" Fonction   : {self.f}")
        print(f" Intervalle : [{a},{b}]")
        print(f" Erreur     : {err} \n")


        if( f(a)*f(b) > 0):
            raise SolverException(" f(a) et f(b) doivent etre de signe different !")

        else:
            debut = a
            fin = b
            n=1

            while (fin - debut > err):
                millieu = (debut + fin) / 2
                x_list.append(millieu)
                print(f"Found solution after {n} iterations : {millieu} ")
                n+=1
                if (f(debut) * f(millieu) < 0):
                    fin = millieu
                else:
                    debut = millieu

            print(f" Solution approchee de f(x) = 0 est : {millieu}\n")
            return x_list
\end{lstlisting}








\newpage
\subsection{Interpretation Géometrique}
L'algorithme de dichotomie est un algorithme classique en algorithmique. Sur un intervalle [a,b] , l'intervalle va à chaque itération se réduire de moitié, puisque soit a soit b prendra la valeur de leur moyenne, soit (a+b)/2. à chaque itération, jusqu'à obtention de l'approximation souhaitée.
\\
\includegraphics[width=13cm,height=8cm]{img/interpretation/dichotomie.png}\\
Les deux premiers pas de la méthode de dichotomie.


\subsection{Convergence}
L'algorithme de dichotomie produit deux suites de valeurs : une suite croissante (qui donne une estimation par le bas de la solution) et une suite décroissante (qui donne une estimation par le haut de la solution). La quantité d’intérêt ici est la longueur de l'intervalle d'incertitude, qui décroît toujours de la même façon.
Ceci rend l'algorithme de dichotomie un bon algorithme "informatif" puisqu'il permet, à n'importe qu'elle itération de l'algorithme, de nous assurer la présence de la solution entre un intervalle [ début, fin ]. On a donc une incertitude explicite et non asymptotique, contrairement à d'autres méthodes que nous verrons comme Newton ou Cordes. 
Ainsi, pour mettre en évidence cette convergence, nous avons tracer un trait rouge (estimation par le bas) et un bleu (estimation par le haut) qui permettront à tout pas de l'algorithme de délimiter cet intervalle, où la solution y est.

De plus, on peut également affirmer que l'algorithme de dichotomie converge sans surprise, mais à une vitesse lente. 
\section{Méthode de Newton}
\subsection{Présentation} 
\quad Afin de mettre au point des algorithmes possédant de meilleures propriétés de convergence que la méthode de dichotomie, il est nécessaire de prendre en
compte les informations données par les valeurs de f et, éventuellement, par sa dérivée f' (si f est différentiable) ou par une approximation convenable de celle-ci.\\
 L'algorithme de la méthode de Newton peut être présenté brièvement comme suit: à chaque itération, la fonction dont on cherche un zéro est linéarisée en l'itéré (ou point) courant et l'itéré suivant est pris égal au zéro de la fonction linéarisée. Cette description sommaire indique qu'au moins deux conditions sont requises pour la bonne marche de l'algorithme : la fonction doit être différentiable aux points visités (pour pouvoir y linéariser la fonction) et les dérivées ne doivent pas s'y annuler (pour que la fonction linéarisée ait un zéro) ; s'ajoute à ces conditions la contrainte forte de devoir prendre le premier itéré assez proche d'un zéro régulier de la fonction (i.e., en lequel la dérivée de la fonction ne s'annule pas), pour que la convergence du processus soit assurée. \\


Ecrivons pour cela le développement de Taylor de f en $\alpha$  au premier ordre.
On obtient alors la version linéarisée du problème
$f(\alpha) = 0 = f(x) + (\alpha - x)f'(\eta)$,
où $\eta$ est entre $\alpha$ et x. L’équation  conduit à la méthode itérative suivante :
$\forall k \geq 0$, étant donné $x^k$ , déterminer $x^{k+1}$ en résolvant l’équation : \\ 
$ f(x^k) + (x^{k+1}-x^k)q_k = 0$, où $q_k$ est une approximation de $f'(x^k)$.

\quadd A present et pour les prochaines méthodes que nous présenterons, nous effectuerons des choix sur $q_k$ . \\
\quadd Ici on pose : \quad $q_k=f'(x_k)  \quad \forall k\geq 0$ \\ et en se donnant la valeur initiale $x^0$ , on obtient la méthode de Newton : \\
    \quadd $x^{k+1} = x^k - \frac{f(x^k)}{f'(x^k)} \quad \forall k\geq 0$ \\

A la k-ème itération, la méthode de Newton nécessite l’évaluation des deux fonctions f et f' au point $x^k$. Cet effort de calcul supplémentaire est plus
que compensé par une accélération de la convergence, la méthode de Newton étant d’ordre 2. 

Or, il est possible que la dérivée de la fonction f soit relativement pénible à calculer et c'est pour ça que nous avons présenté une deuxieme version d'implémentation de cette méthode,sans dérivée donnée en paramètre,puisqu'elle est automatiquement calculée.
Cela rend la méthode de Newton très agréable à utiliser,puisqu'elle converge très rapidement et ne nécessite donc, uniquement un point approximatif $x_0$ comme argument supplementaire à la fonction.

\newpage
\subsection{Interpretation Géometrique}
Autrement dit, on veut approcher la fonction au premier ordre, c'est à dire, on la considère asymptotiquement égale à sa tangente en ce point :

{\displaystyle f(x)\simeq f(x_{0})+f'(x_{0})(x-x_{0}).}

Partant de là, pour trouver un zéro de cette fonction d'approximation, on calcule l'intersection de la droite tangente avec l'axe des abscisses, c'est-à-dire résoudre l'équation affine :

{\displaystyle 0=f(x_{0})+f'(x_{0})(x-x_{0}).}

On obtient alors un point $x^1$ qui en général a de bonnes chances d'être plus proche du vrai zéro de f que le point $x^0$ précédent. Par cette opération, on peut donc espérer améliorer l'approximation par itérations successives (voir illustration) : on approche à nouveau la fonction par sa tangente en {\displaystyle x_{1}} pour obtenir un nouveau point $x^2$, etc.
\\
\includegraphics[width=13cm,height=8cm]{img/interpretation/newton.png}\\
Les deux premieres étapes de la méthode de Newton.
\newpage
\subsection{Implementation du code}
\begin{lstlisting}[language=Python, caption=Méthode de Newton en Python]

class Newton(Equa_Solver):

    def solve_with_df(self):
        f=self.f
        Df=self.df
        max_iter=self.max_iter
        x0=self.x0
        epsilon=self.err
        x_list=[]

        fx = lambda x: eval(str(f))
        dfx = lambda x: eval(str(Df))
        print("\n\nfunction f : ", f, "\n", "Derivative f' : ", Df, "\n", "--------------------------------")
        xn = x0
        for n in range(0, max_iter):
            fxn = fx(xn)
            x_list.append(xn)
            self.affiche_info(n,xn,fxn)
            if abs(fxn) < epsilon:
                print('Found solution after', n, 'iterations.')
                print("the approximate solution is : ",x_list[-1])
                return x_list

            Dfxn = dfx(xn)

            if Dfxn == 0:
                print('Zero derivative. No solution found.')
                return None
            xn = xn - fxn / Dfxn

        print('Exceeded maximum iterations. No solution found.')
        return None
        
    def solve_without_df(self):
        f=self.f
        max_iter=self.max_iter
        x0=self.x0
        epsilon=self.err
        x_list=[]


        x = Symbol('x')
        fx = lambda x: eval(str(f))
        dfx = lambda x: eval(str(diff(f)))
        print("\n\nfunction f : ", f, "\n", "Derivative f' : ", diff(f), "\n", "--------------------------------")

        xn = x0

        for n in range(0, max_iter):
            fxn = fx(xn)
            x_list.append(xn)
            self.affiche_info(n, xn, fxn)
            if abs(fxn) < epsilon:
                print('Found solution after', n, 'iterations.')
                print("the approximate solution is : ",x_list[-1])
                return x_list
                
            Dfxn = dfx(xn)
            if Dfxn == 0:
                print('Zero derivative. No solution found.')
                return None
            xn = xn - fxn / Dfxn
        print('Exceeded maximum iterations. No solution found.')
        return None
\end{lstlisting}

\subsection{Convergence}
L'algorithme produit une suite de valeurs qui convergent vers la solution de l'exercice.En outre on ne connaît pas la vraie solution, la théorie nous dit simplement qui on converge vers elle.
Pour l'implémentation du calcul de la convergence, il faut donc utiliser la définition de cette derniere defini dans l'introduction. 
\begin{lstlisting}[language=Python, caption=Calcul de convergence en Python]
    def rate(x_list, x_final):
        e = [abs(x_ - x_final) for x_ in x_list]
        q = [(log(e[n+1]/e[n]))/(log(e[n]/e[n-1])) for n in range(1, len(e)-1, 1)]
        return q
\end{lstlisting}
A ce jour, la méthode de newton est la méthode qui converge le plus rapidement,s’il y a convergence, parmis celles présentées aujourdh'ui. En effet, celle-ci est rapide (souvent quadratique), et de plus elle nécessite uniquement un seul point de départ (généralement grossièrement proche de la solution).
Mais, malheureusement f doit être suffisamment régulière, la convergence n’est pas assurée dans tous les cas, s’il y a plusieurs racines elle ne converge par forcément vers la plus proche du point de départ. Aussi, la fonction doit être C1 et il faudrait parfois connaître la dérivéé.


\newpage
\section{Méthode des Cordes}
\subsection{Présentation} 
La méthode des cordes est une méthode comparable à celle de Newton, où l'on remplace ${\displaystyle f'(x_{n})\,}, par \quad  {\displaystyle {\frac {f(x_{n})-f(x_{n-1})}{x_{n}-x_{n-1}}}}$ \\ \\
L'initialisation nécessite deux points $x_0$ et $x_1$, proches, si possible, de la solution recherchée. Il n'est pas nécessaire que $x_0$ et $x_1$ encadrent une racine de f. La méthode des cordes peut aussi être vue comme une généralisation de la méthode de la fausse position, où les calculs sont itérés. \\
 Ici on pose donc : $q_k = \frac{f(x^k) - f(x^{k-1})}{x^k - x^{k-1}} \quad \forall k \geq 0 $
 d’où on déduit, en se donnant deux valeurs initiales $x^{−1}$ et $x^0$,la relation suivante : \\ \\
    $x^{k+1} = x^k - \frac{x^k - x^{k-1}}{ f(x^k)- f(x^{k-1}) }f(x^k)$

\subsection{Interpretation Géometrique}
De manière très intuitive, cette méthode consiste à tracer une droite entre les f(a) et f(b), qui passera forcement par l'axe des abscisses en un point,$x^1$ qui sera la prochaine itération. On réitère le procédé jusqu'à approximation de la solution. 

\includegraphics[width=11cm,height=6cm]{img/interpretation/cordes2.png}\\
Sur ce graphe,on peut voir les deux premières étapes de la méthode pour la résolution d'une fonction f. \\

\includegraphics[width=11cm,height=6cm]{img/interpretation/cordes.png}\\
Illustration des deux premières itérations, pour une autre courbe (ici, la méthode va diverger car $x^0$ et $x^1$ sont choisis trop loin de la solution).
\subsection{Implementation du code}
\begin{lstlisting}[language=Python, caption=Méthode des cordes en Python]
    class Cordes(Equa_Solver):
    
    def solve(self):
        f=self.f
        a,b =self.a,self.b
        max_iter=self.max_iter
        epsilon=self.err
        x_list=[]

        fx = lambda x: eval(str(f))
        print("\n\nfunction f : ", f, " dans l'intervalle [", a, ",", b, "] \n", "--------------------------------")


        for n in range(0, max_iter):
            self.affiche_info(n, b, fx(b))
            x_list.append(b)
            if (abs(a - b) < epsilon):
                print('Found solution after', n, 'iterations.')
                return x_list

            z = (a * fx(b) - b * fx(a)) / (fx(b) - fx(a))
            a, b = b, z

        print('Exceeded maximum iterations =', max_iter, '.No solution found.')
        return None
\end{lstlisting}

\subsection{Convergence}
L'algorithme produit une suite de valeurs qui convergent vers la solution de l'exercice.En outre on ne connaît pas la vraie solution, la théorie nous dit simplement que l'on converge vers elle.
Pour l'implémentation du calcul de la convergence, il faut donc utiliser la définition de cette derniere defini dans l'introduction. 
\begin{lstlisting}[language=Python, caption=Calcul de convergence en Python]
    def rate(x_list, x_final):
        e = [abs(x_ - x_final) for x_ in x_list]
        q = [(log(e[n+1]/e[n]))/(log(e[n]/e[n-1])) for n in range(1, len(e)-1, 1)]
        return q
\end{lstlisting}

Si les valeurs initiales $x^0$ et $x^1$ sont suffisamment proches de la solution, la méthode aura un ordre de convergence de
${\displaystyle \varphi ={\frac {1+{\sqrt {5}}}{2}}\simeq 1,618} $ qui est le nombre d'or.
Ce qui est plus rapide que la méthode de dichotomie par exemple.
Sinon, nous avons vu que la méthode peu diverger.

\newpage
\section{Méthode de la Fausse-Position}
\subsection{Présentation} 
C’est une variante de la méthode des cordes dans laquelle, au lieu de prendre la droite passant par les points $ (x^k,f(x^k))$  et $(x^{k - 1}, f(x^{k - 1}))$, on prend celle passant par $(x^k,f(x^k))$ et $(x^k^{'},f(x^k^{'})))$.

Plus précisemment, une fois trouvées deux valeurs $x^{−1}$ et $x^0$ telles que $f(x^{−1})·f(x^0)< 0$, on pose \\
    $x^{k+1} = x^k$  -  $\frac{x^k - x^k^{'}}{f(x^k) -f(x^k^{'}}  \forall k \geq 0 $  \\
Ainsi, les itérations construites sont toutes contenus dans l'intervalle de départ, [$x^{-1}$,$x^0$] , à la différence de la méthode des cordes.

\subsection{Interpretation Géometrique}
\includegraphics[width=13cm,height=8cm]{img/interpretation/fausseposition.png}\\
\newpage
\subsection{Implementation du code}
\begin{lstlisting}[language=Python, caption=Méthode de la Fausse Position en Python]
class FalsePosition(Equa_Solver):

    def solve(self):
        f=self.f
        a,b=self.a,self.b
        tol=self.err
        x_list=[]
        fx = lambda x: eval(str(f))
        print("\n\nfunction f : ", f, " dans l'intervalle [", a, ",", b, "] \n", "--------------------------------")

        if fx(a) * fx(b) > 0:
            raise SolverException(" f(a) et f(b) doivent etre de signe different !")

        n = 0
        while abs(b - a) > 2 * tol:
            c = (a * fx(b) - b * fx(a)) / (fx(b) - fx(a))
            self.affiche_info(n, c, fx(c))
            n += 1

            x_list.append(c)

            if fx(c - tol) * fx(c + tol) <= 0:
                print('Found solution after', n, 'iterations.')
                return x_list
            if fx(a) * fx(c) > 0:
                a = c
            else:
                b = c
            
        print('Found solution after', n, 'iterations.')
        x_list.append((a+b)/2)
        return x_list
\end{lstlisting}
\newpage
\subsection{Convergence}
Pour l'implémentation du calcul de la convergence, il faut donc utiliser la définition de cette derniere defini dans l'introduction. 
\begin{lstlisting}[language=Python, caption=Calcul de convergence en Python]
    def rate(x_list, x_final):
        e = [abs(x_ - x_final) for x_ in x_list]
        q = [(log(e[n+1]/e[n]))/(log(e[n]/e[n-1])) for n in range(1, len(e)-1, 1)]
        return q
\end{lstlisting}
La méthode de la fausse position, bien qu’ayant la même complexité que la méthode des cordes,a une convergence linéaire.
La méthode de la fausse position peut être vue comme une méthode globalement convergente, tout comme celle de dichotomie.
La méthode de la fausse position peut se voir un peu comme un entre-deux , entre les methodes de dichotomie (globalement convergente ) et des cordes ( meilleur taux de convergence).

\newpage
\section{Comparaison des méthodes}
Pour comparer les algorithmes, il est important de tenir compte de la présence ou non des facteurs suivants :
 -assurance de la convergence
 - vitesse de la convergence
 - stabilité de l'algorithme - précision des résultats
 - complexité des calculs (ex : nombre de fonctions à calculer à chaque itération). 
En outre, on peut résumer, en addition à ce que nous avons dit précedemment :
\begin{itemize}
    \item  Méthode de dichotomie : \\
        - Avantages:  \\
        la convergence est assurée,\quad on a un encadrement de la solution , \quad un seul calcul de fonction à chaque itération \\
        -Inconvénients : \\
           vitesse de convergence linéaire, donc lente , \quad
           sensible aux erreurs d'arrondi ; (exemple : la fonction $f(x) = e^x - 1 - x - \frac{x^2}{2} $ s'annule théoriquement en x = 0 seulement. Numériquement, elle change de signe un grand nombre de fois autour de x = 0.
    
    \item  Methode des cordes : \\
             - Avantages:  \\
        nécessite une seule évaluation de fonction à chaque étape\quad , convergence relativement rapide, bien que moins que celle de Newton \\
        -Inconvénients : \\
         comme la méthode de Newton, peut diverger si les données initiales sont mal choisies, \quad
          le calcul de f (xn) − f (xn−1) peut produire des erreurs de chute.
    \item  Methode de Newton : \\
             - Avantages:  \\
        converge rapidement quand elle converge (ce qui compense largement le dernier inconvénient)\quad  relativement stable et peu sensible aux erreurs d'arrondis si $f'(x_{inf})$ n'est pas trop petit. \\
        -Inconvénients : \\
          peut diverger ou converger vers un autre zéro que celui cherché si la donnée initiale
est mal choisie, \quad
           nécessite le calcul de la dérivée d'une fonction, ce qui est numériquement dicile si
on ne la connait pas explicitement , et enfin \quad chaque étape nécessite deux évaluations de fonctions
    
\end{itemize}
    
       

\section{Interface Graphique}
\subsection{Outils utilisés}
Afin de produire une version plus pratique à l'utilisateur plutot que faire un affichage sur console délicat de suivre les résultats avec on a préviligé de réaliser une interface graphique plutot simple mais efficace.\\
La réalisation de cette interface a été effectué à l'aide de la bibliotheque Tkinter des modules déjà existants sur Python et aussi la réalisation des graphes pour pouvoir afficher les courbes et les différents résultats était fait à l'aide de la bilbiotheque Matplotlib qui du fait doit etre installé pour le bon fonctionnement du projet.
\\ \\
Notre application est consitué de 5 principales pages :
\subsection{Accueil}
\includegraphics[width=13cm,height=8cm]{img/accueil.JPG}\\

L'interface d'accueil de notre application est constitué des quatres méthodes déjà citées afin de résoudre \\ des equations f(x)=0 et pour accéder à chaque méthode il suffit de suivre le boutton correspandant \\
à la méthode\\
Alors dans ce qui suit on va détailler chaque page dans notre interface graphique 

\subsection{Dichotomie}
\includegraphics[width=13cm,height=8cm]{img/dicho.JPG}\\

Sur cette page on s'occupe de la méthode de dichotomie donc on prend en parametres :\\
-une fonction f\\
-un interval [a,b]

Alors si l'utilisateur de notre application rentre bien les infomations attendus et puis il clique sur le boutton solve il aura un affichage :\\
\includegraphics[width=13cm,height=7cm]{img/dicho_graph.JPG}\\

Dans la fenetre on remarque bien les informations relatives à la fonction rentrée mais aussi les résultats retournés apres éxecution de la méthode de Dichotomie \\
NB : le tableau affiche les résultats des 10 dernieres itérations de cette méthode  
\subsubsection{Convergence}
Pour la méthode de dichotomie la convergence vers la solution est graphiqueement représentés par des lignes horizontales qui se rapprochent du point cherché comme il suit : \\
\includegraphics[width=13cm,height=7cm]{img/dicho_cv.JPG}\\


\subsection{Newton}
\includegraphics[width=13cm,height=7cm]{img/newton.JPG}\\

Dans cette page on résout les equations f(x)=0 à l'aide de la méthode de Newton donc on prends en arguments une fonction f et optionnellement sa dérivée et aussi un point de départ de notre récurrence  $ x_0 $
et un champ error qui indique avec quelle précision les résultats devront etre pris en compte cette valeur est initialisée à $\num{e-15}$ de base.\\
Apres rensignement des champs et cliquer sur le boutton solve une fentetre va s'afficher :\\
\includegraphics[width=13cm,height=7cm]{img/newton_graph.JPG}\\
avec toutes les information comme déja expliquer dans le paragraphe précédent mais cette fois çi le taux de convergence est explicitement calculé et donné avec les résultats

\subsection{Cordes}
\includegraphics[width=13cm,height=7cm]{img/cordes.JPG}\\

Cette page s'occupe comme son nom l'indique de résolution des equation f(x)=0 à l'aide de la méthode des cordes elle prend en arguments une fonction f et un interval [a,b] et un clique sur le boutton solve nous donne cette fenetre :\\ 
\includegraphics[width=13cm,height=7cm]{img/cordes_graph.JPG}\\

Comme bien illustré sur le graphique on nous permet de suivre la convergence de la solution volu tout en affichant le taux de convergence correspandant avec les informations nécaissaires de la fonction et l'interval.\\
\subsection{Fausse Position}
\includegraphics[width=13cm,height=7cm]{img/fausse_pos.JPG}\\

Cette page s'occupe comme son nom l'indique de résolution des equation f(x)=0 à l'aide de la méthode de fausse position elle prend en arguments une fonction f et un interval [a,b] et un clique sur le boutton solve nous donne cette fenetre : \\
\includegraphics[width=13cm,height=7cm]{img/cordes_graph.JPG}\\

Comme bien illustré sur le graphique on nous permet de suivre la convergence de la solution volu tout en affichant le taux de convergence correspandant avec les informations nécaissaires de la fonction et l'interval.\\
\newpage
\subsection{Gestion des erreurs}
Comme déja cité dans les différentes fentetres de notre application une vérification de données tapées est nécaissaire pour le bon fonctionnement du projet alors les vérifications faites sont :

\subsubsection{Formule Erronée}
\includegraphics[width=13cm,height=6cm]{img/validation/err_fct.JPG}\\

Une formule entrée en champ dédiée est dite erronée si elle est mal exprimée au sens de Python donc toutes les formules sont supposées écrites en syntaxe de Python

\subsubsection{Monotonie}
\includegraphics[width=13cm,height=6cm]{img/validation/err_fa_fb.JPG}\\

Si on rentre une fonction qui ne vérifie pas la condition des hypotheses de théoreme de valeurs intermédiaires une érreur s'affichera indiquant ce fait .

\subsubsection{Interval Erroné}
\includegraphics[width=13cm,height=5cm]{img/validation/err_interval.JPG}\\

Si on rentre un interval qui ne correspand un de ces bornes à des nombres (entiers ou floats) une érreur de type interval erroné s'affichera.

\subsubsection{Max d'itération atteint sans trouver de solution}
\includegraphics[width=13cm,height=8cm]{img/validation/err_max.JPG}\\

Si notre fonction n'admet pas de valeurs qui le rendent nulle dans l'interval donné et donc on atteint le maximum d'itération sans trouver de solution pour f(x)=0 alors on afficher la fentetre d'erreur ci dessous


\newpage
\section{Ouverture : Acceleration de la convergence }
D'une manière générale, étant donnée une suite $x_n$ convergeant vers $x_{inf}$, accélérer sa convergence consiste à la remplacer par une suite $y_n$ convergeant plus vite vers $x_{inf}$, c'est-à-dire vérifant : \\ \\
        $\lim\limits_{n \rightarrow +\infty} \frac{ y_n - x_{inf} }{x_n - x_{inf}}=0$ \\
        \\
Par exemple, si $x_n$ converge linéairement, on cherchera à construire $y_n$ convergeant quadratiquement.

Pour cela, on va utiliser la méthode de "relaxation". Elle est basée sur une idée très simple. Considérons une méthode de point fixe $x_{n+1} = g(x_n)$ qui ne converge pas ou très lentement vers un point fixe x*
\\ L'équation qu'on cherche à résoudre $x = g(x)$ peut également s'écrire : \\
$\alpha x + x = g(x) + \alpha x \quad \forall \alpha \quad réel $  \\

L'écriture de l'équation sous cette forme suggère d'utiliser
une nouvelle méthode de point fixe :
    $x_{n+1} = \frac{g(x_n) + \alpha x_n}{ \alpha + 1 }:= G(xn)$ \\
  Ceci  va converger (en choisissant
d'initialiser assez près de la solution) dès que :
    $|G'(x*)|=|\frac{g'(x*) + \alpha }{\alpha + 1} | < 1$ et ce d'autant plus rapidement que  $|\frac{g'(x*) + \alpha }{\alpha + 1} |$ est petit. \\
    Comme on est parfaitement libre de choisir le paramètre $\alpha$ (qui s'appelle ici paramètre de relaxation), l'idée est de le prendre le plus proche possible de −g'(x*).\\ Bien sûr, la valeur exacte de g(x*) n'est en général pas connue, mais par encadrement on peut en avoir des valeurs approchées convenables, qu'on peut d'ailleurs réactualiser éventuellement au cours des itérations. En conclusion cette méthode est très simple d'utilisation et peut rendre de précieux services. Mais avant tout , il faudrait étudier la méthode des points fixes qui n'a pas été étudié ici.
\newpage
\section{Conclusion}
    Ainsi, nous avos pu étudier quelques méthodes utiles pour la résolution d'équations non linéaires en les présentant chacun avec leur caracteristiques propres, leurs avantages et leurs inconvénients.
    
    L'interface graphique, qui a été conçu pour être la plus facilement utilisable et fiable, permet de tester chacune de ces méthodes et d'y avoir une visualisation graphique mais aussi en interface terminale pour plus de détails sur les itérations. De plus, nous avons préparé un fichier test permettant de comparer chacune des méthodes sur 5 résolutions différentes.
    Pour comparer les vitesses de convergence des méthodes vues précedemment, nous pouvons le voir sur un exemple précis : \\
    \includegraphics[width=13cm,height=8cm]{img/interpretation/convergence.png}\\
historique des convergences de la résolution de $f(x) = cos^2 (2x) - x^2$ sur l’intervalle ]0, 1.5[ pour les méthodes de dichotomie (2), des cordes (3) et de Newton (4). Le nombre d’itérations est reporté sur l’axe des x et l’erreur absolue sur l’axe des y

    Cependant, un bon choix qui peut sembler optimal est de combiner certaines de ces méthodes comme par exemple, utiliser la dichotomie qui est lente mais sans surprise pour trouver une premiere approximation grossiere , avant d'utiliser cette derniere comme point d'entrée pour la méthode de Newton qui est rapide et nécessite justement, ce $x_0$ "grossier".
    
    Comme ouverture théorique, nous pouvons voir également des moyens d'accelerations de la convergence. 
\end{document}